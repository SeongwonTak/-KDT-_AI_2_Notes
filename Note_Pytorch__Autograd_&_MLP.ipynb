{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Note/Pytorch_ Autograd & MLP.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOZ4mmyHQwk5z39t3pNG/FU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SeongwonTak/-KDT-_AI_2_Notes/blob/main/Note_Pytorch__Autograd_%26_MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xq57RDEKlIxy"
      },
      "source": [
        "# Pytorch_ Autograd & MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3y5Lq44lCZU",
        "outputId": "b8eb7117-11c1-454a-b4a8-bf9bfc2097ec"
      },
      "source": [
        "import torch\n",
        "\n",
        "# 연산 과정의 추적 : requires_grad = True\n",
        "x = torch.ones(2, 2, requires_grad = True)\n",
        "print(x)\n",
        "\n",
        "print(x.grad_fn) # 최초 자체 생성으로 None이 된다."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBcdIQ69lWSM",
        "outputId": "efe42089-fad2-43cc-d57d-df813dbf24ad"
      },
      "source": [
        "y = x + 2\n",
        "print(y)\n",
        "print(y.grad_fn) # 연산의 결과이므로 grad_fn이 생긴다."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n",
            "<AddBackward0 object at 0x7eff5eb75f10>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bp_-BG6TlduY",
        "outputId": "077eaefd-575b-4093-f639-dc3b47767ce2"
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z)\n",
        "print(out)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>)\n",
            "tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYg4jE2slopc",
        "outputId": "0a7d7fe1-8958-4385-e1aa-4cfb97271aa4"
      },
      "source": [
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3) / (a - 1))\n",
        "print(a)\n",
        "print(a.requires_grad) # 미생성시 False로\n",
        "a.requires_grad_(True) # True로 바꾸면?\n",
        "print(a.requires_grad) # 성공\n",
        "b = (a * a).sum()\n",
        "print(b)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[ 0.6491,  1.1257],\n",
            "        [-4.2669, -2.6000]])\n",
            "False\n",
            "True\n",
            "tensor(26.6551, grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GEhLMy9mpi8"
      },
      "source": [
        "## Gradient와 역전파의 진행"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PjKDyrEll736"
      },
      "source": [
        "y.retain_grad()\n",
        "z.retain_grad()\n",
        "out.backward()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "id": "9TUOusf8mz0V",
        "outputId": "97123bdd-e3e8-4c73-c07a-f1fadc3aea09"
      },
      "source": [
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)\n",
        "print(z.is_leaf) # z가 leaf인가 아닌가?\n",
        "\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[0.2500, 0.2500],\n",
            "        [0.2500, 0.2500]])\n",
            "False\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-e802eeed1313>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_leaf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved variables after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved variables after calling backward."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KsVzCsuSm62n",
        "outputId": "512e935e-e3c2-44ae-c04b-8b36b42df54e"
      },
      "source": [
        "x = torch.ones(2, 2, requires_grad=True)\n",
        "y = x + 2\n",
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(out)\n",
        "\n",
        "y.retain_grad()\n",
        "out.backward(retain_graph=True)\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)       # z.retain_grad()를 호출하지 않으면 grad값을 저장하지 않기 때문에 grad 속성을 볼 수 없음\n",
        "print(z.is_leaf)\n",
        "\n",
        "out.backward()\n",
        "print(x.grad)\n",
        "print(y.grad)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(27., grad_fn=<MeanBackward0>)\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n",
            "None\n",
            "False\n",
            "tensor([[9., 9.],\n",
            "        [9., 9.]])\n",
            "tensor([[9., 9.],\n",
            "        [9., 9.]])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:13: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more information.\n",
            "  del sys.path[0]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cno5TWcznIcq",
        "outputId": "05105b19-33ed-46f9-e90e-b5a3a8ae0d8d"
      },
      "source": [
        "x = torch.randn(3, requires_grad=True)\n",
        "y = x * 2\n",
        "\n",
        "while y.data.norm() < 1000:\n",
        "  y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([ 872.3803,  842.2120, 1169.7627], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eNeZr65n0qB",
        "outputId": "d067d21c-c033-494a-b63e-ff5c6d760be6"
      },
      "source": [
        "# scalar값이 아닌 y의 벡터-야코비안 곱을 구하는 과정\n",
        "v = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(v)\n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJx8hO73oVdx"
      },
      "source": [
        "# ANN 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UKYuenYon3SW"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lJGNV_WToZSL"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer0 = nn.Linear(4, 128)\n",
        "        self.layer1 = nn.Linear(128, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(16, 3)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(128) #값들을 정규화 한다.\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.act(self.bn0(self.layer0(x)))\n",
        "      x = self.act(self.bn1(self.layer1(x)))\n",
        "      x = self.act(self.bn2(self.layer2(x)))\n",
        "      x = self.act(self.layer3(x))\n",
        "      x = self.layer4(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "el-wg3--o1UP",
        "outputId": "a721a670-d4c4-43bf-af4b-cbdb10827fb8"
      },
      "source": [
        "# 랜덤 값 생성\n",
        "criterion = nn.CrossEntropyLoss() # 오차\n",
        "\n",
        "ex_X, ex_y = torch.randn([4, 4]), torch.tensor([1, 0, 2, 0])\n",
        "\n",
        "net = Net()\n",
        "output = net(ex_X)\n",
        "loss = criterion(output, ex_y)\n",
        "print('loss: ', loss.item())\n",
        "  \n",
        "net.zero_grad() # 역전파를 위해 grad 초기화.\n",
        "\n",
        "print('layer0.bias.grad before backward')\n",
        "print(net.layer4.bias.grad)\n",
        "\n",
        "print(net.layer4.bias.is_leaf)\n",
        "\n",
        "loss.backward()\n",
        "\n",
        "print('layer0.bias.grad after backward')\n",
        "print(net.layer4.bias.grad)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss:  1.1805709600448608\n",
            "layer0.bias.grad before backward\n",
            "None\n",
            "True\n",
            "layer0.bias.grad after backward\n",
            "tensor([-0.2529,  0.2101,  0.0428])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w5LLNULpTLe"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.001)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "output = net(ex_X)\n",
        "loss = criterion(output, ex_y)\n",
        "loss.backward()\n",
        "optimizer.step()  # 업데이트 진행"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8R5PzdQVp9cw"
      },
      "source": [
        "## Example - Iris."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCmR3NONppGE"
      },
      "source": [
        "dataset = load_iris()\n",
        "\n",
        "data = dataset.data\n",
        "label = dataset.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(data, label, test_size=0.25)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3TO8gFNqTks"
      },
      "source": [
        "# DataLoader 생성\n",
        "X_train = torch.from_numpy(X_train).float()\n",
        "y_train = torch.from_numpy(y_train).long()\n",
        "\n",
        "X_test = torch.from_numpy(X_test).float()\n",
        "y_test = torch.from_numpy(y_test).long()\n",
        "\n",
        "train_set = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_set, batch_size=4, shuffle=True)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-86VFPqGRM"
      },
      "source": [
        "class Net(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "\n",
        "        self.layer0 = nn.Linear(4, 128)\n",
        "        self.layer1 = nn.Linear(128, 64)\n",
        "        self.layer2 = nn.Linear(64, 32)\n",
        "        self.layer3 = nn.Linear(32, 16)\n",
        "        self.layer4 = nn.Linear(16, 3)\n",
        "\n",
        "        self.bn0 = nn.BatchNorm1d(128)\n",
        "        self.bn1 = nn.BatchNorm1d(64)\n",
        "        self.bn2 = nn.BatchNorm1d(32)\n",
        "\n",
        "        self.act = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.act(self.bn0(self.layer0(x)))\n",
        "      x = self.act(self.bn1(self.layer1(x)))\n",
        "      x = self.act(self.bn2(self.layer2(x)))\n",
        "      x = self.act(self.layer3(x))\n",
        "      x = self.layer4(x)\n",
        "\n",
        "      return x"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jSC5cI-7qJkc"
      },
      "source": [
        "net = Net()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "epochs = 200"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC3hdFv-qMlt"
      },
      "source": [
        "losses = list()\n",
        "accuracies = list()\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  epoch_loss = 0  \n",
        "  epoch_accuracy = 0\n",
        "  for X, y in train_loader:\n",
        "  \n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    output = net(X)\n",
        "\n",
        "    loss = criterion(output, y)\n",
        "    loss.backward()\n",
        "    \n",
        "    optimizer.step()\n",
        "    # output = [0.11, 0.5, 0.8]  --> 예측 클래스 값\n",
        "    _, predicted = torch.max(output, dim=1) # 최대값의 index를 뱉는다.\n",
        "    accuracy = (predicted == y).sum().item()\n",
        "    epoch_loss += loss.item()\n",
        "    epoch_accuracy += accuracy\n",
        "  \n",
        "\n",
        "  epoch_loss /= len(train_loader)\n",
        "  epoch_accuracy /= len(X_train)\n",
        "  print(\"epoch :{}, \\tloss :{}, \\taccuracy :{}\".format(str(epoch+1).zfill(3),round(epoch_loss,4), round(epoch_accuracy,4)))\n",
        "  \n",
        "  losses.append(epoch_loss)\n",
        "  accuracies.append(epoch_accuracy)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X34nmneuqPsk",
        "outputId": "52bb4616-fc60-4666-cb92-4c28cd7d719b"
      },
      "source": [
        "# Test\n",
        "\n",
        "output = net(X_test)\n",
        "print(torch.max(output, dim=1))\n",
        "_, predicted = torch.max(output, dim=1)\n",
        "accuracy = round((predicted == y_test).sum().item() / len(y_test),4)\n",
        "\n",
        "\n",
        "print(\"test_set accuracy :\", round(accuracy,4))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.return_types.max(\n",
            "values=tensor([0.8116, 1.8552, 0.8430, 2.6419, 1.2063, 3.0256, 0.9870, 1.1213, 0.9885,\n",
            "        0.5825, 2.3526, 2.0756, 2.2890, 1.0254, 1.8342, 0.9891, 1.3011, 2.4157,\n",
            "        1.9383, 2.3550, 2.2660, 2.8542, 1.0786, 0.8992, 1.3460, 1.9769, 2.3171,\n",
            "        1.2148, 1.8115, 1.6862, 1.3565, 2.6731, 2.6688, 0.9835, 1.6969, 2.3228,\n",
            "        0.7760, 0.8229], grad_fn=<MaxBackward0>),\n",
            "indices=tensor([1, 2, 2, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 2, 0, 2, 0, 1, 1,\n",
            "        2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 1, 1]))\n",
            "test_set accuracy : 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymnTnXQVqe8G"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}